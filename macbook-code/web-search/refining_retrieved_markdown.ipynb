{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Cleaned Text:  [Jump to content]( From Wikipedia, the free encyclopedia Walmart has been the world's largest company by revenue since 2014. 673.82B revenue) # Biggest U.S. Companies by Revenue 1. [WMT] Walmart Inc. 673.82B 2. [AMZN] Amazon.com, Inc. 620.13B 3. [UNH] UnitedHealth Group 393.90B 4. [AAPL] Apple Inc. 391.04B 5. [BRK.B] Berkshire Hathaway Inc. 369.89B \n",
      "\n",
      "ðŸ”¹ Relevant Chunks for LLM Context: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ftfy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load Sentence Embedding Model for relevance-based ranking\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing markdown, HTML tags, and unnecessary noise.\n",
    "    \"\"\"\n",
    "    text = ftfy.fix_text(text)  # Fix encoding issues\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove Emails\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags using BeautifulSoup\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "def extract_companies(text):\n",
    "    \"\"\"\n",
    "    Extracts company names, descriptions, and other metadata from the raw text.\n",
    "    \"\"\"\n",
    "    company_pattern = re.compile(r\"(\\d+[\\.)\\-]?\\s*)?([A-Za-z&\\.\\-\\s]+(?:Inc|Ltd|Corp|Group|Corporation)?)\\s*(?:\\(|:)?([\\w\\s,\\.%&\\-]+)?(?:revenue|sales|income)?[\\:|â€”]?\\s*(\\d+(?:\\.\\d+)?\\s*(million|billion|USD|dollars)?)?\", re.MULTILINE)\n",
    "\n",
    "    companies = []\n",
    "    for match in company_pattern.finditer(text):\n",
    "        # Extract matched groups, handling cases where fewer than 4 groups are captured\n",
    "        groups = match.groups()\n",
    "\n",
    "        # Ensure we don't unpack more or fewer than expected\n",
    "        if len(groups) == 4:\n",
    "            index, name, description, revenue = groups\n",
    "        else:\n",
    "            index, name, description, revenue = None, groups[0], groups[1] if len(groups) > 1 else None, None\n",
    "\n",
    "        name = name.strip() if name else \"Unknown Company\"\n",
    "        description = description.strip() if description else \"No description\"\n",
    "        revenue = revenue.strip() if revenue else \"Unknown\"\n",
    "\n",
    "        company_info = {\n",
    "            \"name\": name,\n",
    "            \"description\": description,\n",
    "            \"revenue\": revenue\n",
    "        }\n",
    "        companies.append(company_info)\n",
    "\n",
    "    return companies\n",
    "\n",
    "def filter_companies_by_query(companies, query):\n",
    "    \"\"\"\n",
    "    Filters companies based on a query. The query may reference any kind of company data (revenue, market cap, industry, etc.)\n",
    "    \"\"\"\n",
    "    filtered_companies = []\n",
    "    \n",
    "    # For now, filter based on revenue mentioned in the query\n",
    "    revenue_match = re.search(r\"(\\d+\\.?\\d*)\\s*(billion|million)?\\s*dollar\", query, re.IGNORECASE)\n",
    "    min_revenue_million = 500  # Default: 500 million\n",
    "    if revenue_match:\n",
    "        min_revenue_million = float(revenue_match.group(1))\n",
    "        if revenue_match.group(2) and \"billion\" in revenue_match.group(2).lower():\n",
    "            min_revenue_million *= 1000  # Convert billion to million\n",
    "\n",
    "    for company in companies:\n",
    "        revenue = company[\"revenue\"]\n",
    "        if revenue and revenue != \"Unknown\" and revenue.lower().find(\"billion\") >= 0:\n",
    "            revenue_value = float(revenue.split()[0].replace(\",\", \"\"))\n",
    "            if revenue_value >= min_revenue_million:\n",
    "                company[\"revenue_million\"] = revenue_value\n",
    "                filtered_companies.append(company)\n",
    "\n",
    "    return filtered_companies\n",
    "\n",
    "def preprocess_text(text, query):\n",
    "    \"\"\"\n",
    "    Cleans the text, extracts relevant company data, and organizes it to be used as context for LLM-based queries.\n",
    "    \"\"\"\n",
    "    # Step 1: Clean and Normalize Text\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Step 2: Extract Companies and Descriptions\n",
    "    companies = extract_companies(cleaned_text)\n",
    "\n",
    "    # Step 3: Filter Companies Based on Query\n",
    "    filtered_companies = filter_companies_by_query(companies, query)\n",
    "\n",
    "    # Step 4: Rank the relevance of extracted chunks for context\n",
    "    relevant_chunks = []\n",
    "\n",
    "    # Creating relevant chunks for LLM context, considering description and other details\n",
    "    for company in filtered_companies:\n",
    "        relevant_chunk = f\"{company['name']}: {company['description']} with revenue {company['revenue']}\"\n",
    "        relevant_chunks.append(relevant_chunk)\n",
    "\n",
    "    return {\n",
    "        \"cleaned_text\": cleaned_text,\n",
    "        \"relevant_chunks\": relevant_chunks\n",
    "    }\n",
    "\n",
    "# Example Raw Noisy Text (from crawled chunks)\n",
    "raw_text = \"\"\"\n",
    "[Jump to content]( From Wikipedia, the free encyclopedia Walmart has been the world's largest company by revenue since 2014. 673.82B revenue)\n",
    "# Biggest U.S. Companies by Revenue\n",
    "1. [WMT] Walmart Inc. 673.82B\n",
    "2. [AMZN] Amazon.com, Inc. 620.13B\n",
    "3. [UNH] UnitedHealth Group 393.90B\n",
    "4. [AAPL] Apple Inc. 391.04B\n",
    "5. [BRK.B] Berkshire Hathaway Inc. 369.89B\n",
    "\"\"\"\n",
    "\n",
    "query = \"List all companies with 500 million dollar above revenue\"\n",
    "\n",
    "# Process the text to extract companies with the required revenue\n",
    "processed_output = preprocess_text(raw_text, query)\n",
    "\n",
    "# Display the output\n",
    "print(\"\\nðŸ”¹ Cleaned Text:\", processed_output[\"cleaned_text\"])\n",
    "print(\"\\nðŸ”¹ Relevant Chunks for LLM Context:\", processed_output[\"relevant_chunks\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "crawled_markdowns = []\n",
    "with open('crawled_markdowns.json') as f:\n",
    "    crawled_markdowns = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_chunks = []\n",
    "context_tokens = 0\n",
    "for item in crawled_markdowns[0]['crawled_markdowns_list']:\n",
    "    query = crawled_markdowns[0]['query']\n",
    "    result = preprocess_text(item, query)\n",
    "    context_tokens += len(result['cleaned_text'].split())\n",
    "    cleaned_chunks.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cleaned_chunks[0]['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24971"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper3k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maiohttp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnewspaper3k\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Article\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newspaper3k'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from newspaper3k import Article\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper3k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maiohttp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnewspaper3k\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Article\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newspaper3k'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (4.12.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (2.32.3)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (5.1.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
      "Requirement already satisfied: six in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from requests>=2.10.0->newspaper3k) (2024.12.14)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/ibnabeeali/miniconda3/envs/graphRAG/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
