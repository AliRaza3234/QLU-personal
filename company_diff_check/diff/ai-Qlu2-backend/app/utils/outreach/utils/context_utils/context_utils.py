import os
import json

from copy import deepcopy
from app.utils.outreach.utils.context_utils.context_prompts import (
    HALLUCINATION_CHECK_SYSTEM_PROMPT,
    FOLLOWUP_CHECK_USER_PROMPT,
    CONTEXT_CHECK_SYSTEM_PROMPT,
    FOLLOWUP_CHECK_SYSTEM_PROMPT,
)
from app.utils.outreach.utils.gpt_utils.gpt_runner import gpt_runner

OPEN_API_KEY = os.getenv("OPENAI_API_KEY")
# GPT_MAIN_MODEL = os.getenv("GPT_MAIN_MODEL")
GPT_MAIN_MODEL = "gpt-4o"
# GPT_COST_EFFICIENT_MODEL = os.getenv("GPT_COST_EFFICIENT_MODEL")
GPT_COST_EFFICIENT_MODEL = "gpt-4o-mini"


async def evaluate_hallucination(
    gen_text: str, ref_text: str, profile_summary: str
) -> str:
    """
    Asynchronously evaluates whether the generated text contains hallucinations (inaccurate or fabricated details)
    by comparing it against a reference text and a profile summary.

    This function takes generated text and assesses its accuracy by cross-referencing it with provided reference
    text and a profile summary. It uses a GPT model configured with a system prompt that defines the rules for
    identifying hallucinations. The model then produces an evaluation based on these inputs, indicating if and
    where the generated text deviates inaccurately from the reference materials.

    Parameters:
        gen_text (str): The text generated by a system that needs to be evaluated for accuracy.
        ref_text (str): The reference text used as a basis to verify the generated text.
        profile_summary (str): A summary profile that provides additional context for evaluating the generated text.

    Returns:
        str: A report from the model indicating the presence of any hallucinations in the generated text.

    Note:
        The function outputs a detailed evaluation that helps in identifying areas where the generated text
        might have introduced information not supported by the reference texts, crucial for applications
        requiring high fidelity and accuracy in text generation.
    """
    system_prompt = deepcopy(HALLUCINATION_CHECK_SYSTEM_PROMPT)
    user_prompt = f"""
    Generated Text: {gen_text}
    Reference Text: {ref_text}
    Profile Summary: {profile_summary}
    """
    chat = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    response = await gpt_runner(model=GPT_MAIN_MODEL, temperature=0.2, chat=chat)
    # print("HEREREREER",response)
    return response


# async def evaluate_context(gen_text: str, ref_text: str, profile_summary: str) -> str:
#     """
#     Asynchronously evaluates the contextual relevance and consistency of generated text
#     against a provided reference text and a profile summary.

#     This function uses a GPT model to determine how well the generated text adheres to the contextual
#     expectations set by the reference text and any additional context provided in a profile summary.
#     It does so by configuring the model with a specific system prompt designed to assess context alignment,
#     and then prompts the model with the generated text, reference text, and profile summary. The model's
#     response indicates the degree of contextual relevance or any discrepancies.

#     Parameters:
#         gen_text (str): The text generated by a system that needs to be evaluated for contextual relevance.
#         ref_text (str): The reference text used as a benchmark for context comparison.
#         profile_summary (str): A summary that provides contextual background which aids in the evaluation.

#     Returns:
#         str: A detailed assessment from the model regarding the contextual alignment of the generated text with the reference and profile summary.

#     Note:
#         The output can help identify areas where the generated text may not fully align with the expected context,
#         which is critical for applications where precise context matching is necessary.
#     """
#     system_prompt = deepcopy(CONTEXT_CHECK_SYSTEM_PROMPT)

#     user_prompt = f"""
#     Generated Text: {gen_text}
#     Reference Text: {ref_text}
#     Profile Summary: {profile_summary}
#     """
#     chat = [
#         {"role": "system", "content": system_prompt},
#         {"role": "user", "content": user_prompt},
#     ]
#     response = await gpt_runner(
#         model=GPT_MAIN_MODEL, temperature=0.1, chat=chat
#     )
#     response = response.replace("Score:","")
#     response = response.replace("Score","")
#     return response

# async def rerunning_for_context(system_prompt: str, user_prompt: str, response: str) -> str:
#     """
#     Asynchronously reruns a text generation process to improve or modify the context in the initial response,
#     specifically to include all relevant company names from a reference text.

#     This function is designed to enhance the accuracy and completeness of a generated text by ensuring that
#     it includes all relevant details that may have been omitted in the initial run. It takes the original system
#     and user prompts, along with the initial response, and re-engages the model with an additional prompt to
#     correct specific deficiencies (e.g., missing company names).

#     Parameters:
#         system_prompt (str): The original system prompt used to guide the text generation model.
#         user_prompt (str): The original user prompt that outlines the task for the model.
#         response (str): The initial response generated by the model that needs modification or enhancement.

#     Returns:
#         str: The revised text after the model has been prompted to include specific missing details.

#     Note:
#         This function is particularly useful in iterative text generation processes where precision and
#         specificity are critical, and allows for incremental improvements in text output based on feedback.
#     """
#     context_adding_prompt = """
#     You did not mention all relevant company names from the reference text.\nYou must Mention all relevant company names from reference text.
#     For reference, consider Qlu or Qlu.ai as a company.
#     If there are no companies to mention, return True.
#     If there is nothing else to add, return True
#     """
#     chat = [
#         {"role": "system", "content": system_prompt},
#         {"role": "user", "content": user_prompt},
#         {"role":"assistant","content": response},
#         {"role":"user","content":context_adding_prompt}
#     ]
#     response2 = await gpt_runner(chat, temperature=0.3, model=GPT_MAIN_MODEL)
#     if "true" in response2.lower():
#         response2 = response
#     # print("RESPONSE AFTER RE RUNNING", response)
#     return response2


async def evaluate_context(
    gen_text: str,
    ref_text: str,
    profile_summary: str,
    company: str,
    link: str,
    contact: str,
) -> str:
    """
    Asynchronously evaluates the contextual relevance and consistency of generated text
    against a provided reference text and a profile summary.

    This function uses a GPT model to determine how well the generated text adheres to the contextual
    expectations set by the reference text and any additional context provided in a profile summary.
    It does so by configuring the model with a specific system prompt designed to assess context alignment,
    and then prompts the model with the generated text, reference text, and profile summary. The model's
    response indicates the degree of contextual relevance or any discrepancies.

    Parameters:
        gen_text (str): The text generated by a system that needs to be evaluated for contextual relevance.
        ref_text (str): The reference text used as a benchmark for context comparison.
        profile_summary (str): A summary that provides contextual background which aids in the evaluation.

    Returns:
        str: A detailed assessment from the model regarding the contextual alignment of the generated text with the reference and profile summary.

    Note:
        The output can help identify areas where the generated text may not fully align with the expected context,
        which is critical for applications where precise context matching is necessary.
    """
    system_prompt = deepcopy(CONTEXT_CHECK_SYSTEM_PROMPT)

    user_prompt = f"""
    <generated text> {gen_text} </generated_text>
    <reference text> {ref_text} </reference text>
    """
    if company:
        user_prompt += f"\nCompany to include: {company}"
    if link:
        user_prompt += f"\nLink to include: {link}"
    if contact:
        user_prompt += f"\nContact Info to include: {contact}"
    chat = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    response = await gpt_runner(
        model=GPT_COST_EFFICIENT_MODEL, temperature=0.1, chat=chat, json_format=True
    )
    # response = response.replace("Score:", "")
    # response = response.replace("Score", "")
    response = json.loads(response)
    # print(response)
    return response["score"]


async def rerunning_for_context(
    system_prompt: str, user_prompt: str, response: str
) -> str:
    """
    Asynchronously reruns a text generation process to improve or modify the context in the initial response,
    specifically to include all relevant company names from a reference text.

    This function is designed to enhance the accuracy and completeness of a generated text by ensuring that
    it includes all relevant details that may have been omitted in the initial run. It takes the original system
    and user prompts, along with the initial response, and re-engages the model with an additional prompt to
    correct specific deficiencies (e.g., missing company names).

    Parameters:
        system_prompt (str): The original system prompt used to guide the text generation model.
        user_prompt (str): The original user prompt that outlines the task for the model.
        response (str): The initial response generated by the model that needs modification or enhancement.

    Returns:
        str: The revised text after the model has been prompted to include specific missing details.

    Note:
        This function is particularly useful in iterative text generation processes where precision and
        specificity are critical, and allows for incremental improvements in text output based on feedback.
    """

    context_adding_prompt = """
    1. You have either missed out some important information from the reference text and have failed to effectively maintain context or have added some unnecessary information which was not provided in reference. 
2. Make sure you've covered everything from reference that is important for context such as important links, company names, contact information, or receiver's educational and work experience.    
3. You must Mention all relevant company names, links, contact info and the main key context from reference text. 
4. If something is not present, do not add it yourself.    
5. If there are no companies or links to mention, return True.   
6. If there is nothing else to add, return True
    """
    chat = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
        {"role": "assistant", "content": response},
        {"role": "user", "content": context_adding_prompt},
    ]
    # print(chat)
    response2 = await gpt_runner(chat, temperature=0.3, model=GPT_MAIN_MODEL)
    if "true" in response2.lower():
        response2 = response
    # print("RESPONSE AFTER RE RUNNING", response)
    return response2


async def checkFollow_up(text: str) -> bool:
    """
    Async function to check, given a message, whether it is a follow up message or not.

    Args:
    - Text (str): The message which has to be checked

    Returns:
    - Bool: True if followup, False otherwisew
    """
    systemPrompt = deepcopy(FOLLOWUP_CHECK_SYSTEM_PROMPT)
    userPrompt = deepcopy(FOLLOWUP_CHECK_USER_PROMPT)
    userPrompt = userPrompt.format(text=text)
    chat = [
        {"role": "system", "content": systemPrompt},
        {"role": "user", "content": userPrompt},
    ]
    response = await gpt_runner(
        model=GPT_COST_EFFICIENT_MODEL, temperature=0.1, chat=chat, json_format=True
    )
    response = json.loads(response)
    if response["follow_up"] == "true" or response["follow_up"] == "True":
        response["follow_up"] = True
    elif response["follow_up"] == "false" or response["follow_up"] == "False":
        response["follow_up"] = False
    return response["follow_up"]
